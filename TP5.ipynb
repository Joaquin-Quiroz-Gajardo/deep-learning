{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1icTWjfS52B",
        "outputId": "f04e7bfa-1a6f-4829-ce4d-599091584496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(56000, 28, 28)\n",
            "[0 1 2 3 4 5 6 7 8 9]\n",
            "train: 56000 \n",
            "val: 7000 \n",
            "test: 7000\n",
            "[1,   200] loss: 0.532\n",
            "[2,   200] loss: 0.122\n",
            "[3,   200] loss: 0.088\n",
            "[4,   200] loss: 0.072\n",
            "[5,   200] loss: 0.061\n",
            "[6,   200] loss: 0.052\n",
            "[7,   200] loss: 0.046\n",
            "[8,   200] loss: 0.042\n",
            "[9,   200] loss: 0.037\n",
            "[10,   200] loss: 0.034\n",
            "[11,   200] loss: 0.029\n",
            "[12,   200] loss: 0.027\n",
            "[13,   200] loss: 0.025\n",
            "[14,   200] loss: 0.022\n",
            "[15,   200] loss: 0.020\n",
            "[16,   200] loss: 0.019\n",
            "[17,   200] loss: 0.016\n",
            "[18,   200] loss: 0.016\n",
            "[19,   200] loss: 0.014\n",
            "[20,   200] loss: 0.013\n",
            "[21,   200] loss: 0.011\n",
            "[22,   200] loss: 0.010\n",
            "[23,   200] loss: 0.009\n",
            "[24,   200] loss: 0.009\n",
            "[25,   200] loss: 0.008\n",
            "[26,   200] loss: 0.006\n",
            "[27,   200] loss: 0.006\n",
            "[28,   200] loss: 0.006\n",
            "[29,   200] loss: 0.005\n",
            "[30,   200] loss: 0.004\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataloader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train = pickle.load( open( \"/content/drive/MyDrive/uba/deep learning/train.pkl\", \"rb\" ) )\n",
        "train_labels = pickle.load( open( \"/content/drive/MyDrive/uba/deep learning/train_label.pkl\", \"rb\" ) )\n",
        "\n",
        "print(train.shape)\n",
        "print(np.unique(train_labels))\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "tr = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "class YourDataset(Dataset):\n",
        "\n",
        "    def __init__(self, X_Train, Y_Train, transform=None):\n",
        "        self.X_Train = X_Train\n",
        "        self.Y_Train = Y_Train\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_Train)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        x = self.X_Train[idx]\n",
        "        y = self.Y_Train[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "            y = self.transform(y)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "file = open('/content/drive/MyDrive/uba/deep learning/train.pkl', 'rb')\n",
        "X_train = pickle.load(file)\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_train = X_train.swapaxes(1, 3)\n",
        "\n",
        "file.close()\n",
        "\n",
        "file = open('/content/drive/MyDrive/uba/deep learning/train_label.pkl', 'rb')\n",
        "Y_train = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "train_dataset = YourDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
        "file = open('/content/drive/MyDrive/uba/deep learning/val.pkl', 'rb')\n",
        "X_val = pickle.load(file)\n",
        "X_val = X_val[..., np.newaxis]\n",
        "X_val = X_val.swapaxes(1, 3)\n",
        "\n",
        "file.close()\n",
        "\n",
        "file = open('/content/drive/MyDrive/uba/deep learning/val_label.pkl', 'rb')\n",
        "Y_val = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "val_dataset = YourDataset(X_val, Y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
        "\n",
        "file = open('/content/drive/MyDrive/uba/deep learning/test.pkl', 'rb')\n",
        "X_test = pickle.load(file)\n",
        "X_test = X_test[..., np.newaxis]\n",
        "X_test = X_test.swapaxes(1, 3)\n",
        "\n",
        "file.close()\n",
        "\n",
        "file = open('/content/drive/MyDrive/uba/deep learning/test_label.pkl', 'rb')\n",
        "Y_test = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "test_dataset = YourDataset(X_test, Y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
        "\n",
        "print(\"train:\", len(train_dataset), \"\\nval:\", len(val_dataset), \"\\ntest:\", len(test_dataset))\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3,stride=1,padding=\"same\")\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=3,stride=1,padding=\"same\")\n",
        "        self.fc1 = nn.Linear(16 * 7 * 7, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = Model()\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(30):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs.float())\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = model(images.float())\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ABwU9lkkbS_",
        "outputId": "1acdeada-2a00-41e7-af53-411b3896e85c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 98 %\n"
          ]
        }
      ]
    }
  ]
}